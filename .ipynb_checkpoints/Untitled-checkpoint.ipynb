{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"2\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Module/Package Handle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pandas</th>\n",
       "      <td>pd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>numpy</th>\n",
       "      <td>np</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matplotlib</th>\n",
       "      <td>mpl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matplotlib.pyplot</th>\n",
       "      <td>plt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seaborn</th>\n",
       "      <td>sns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scip.stats.</th>\n",
       "      <td>sts</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Module/Package Handle\n",
       "pandas                               pd\n",
       "numpy                                np\n",
       "matplotlib                          mpl\n",
       "matplotlib.pyplot                   plt\n",
       "seaborn                             sns\n",
       "scip.stats.                         sts"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import JMI_MVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"2\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Module/Package Handle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pandas</th>\n",
       "      <td>pd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>numpy</th>\n",
       "      <td>np</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matplotlib</th>\n",
       "      <td>mpl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matplotlib.pyplot</th>\n",
       "      <td>plt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seaborn</th>\n",
       "      <td>sns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scip.stats.</th>\n",
       "      <td>sts</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Module/Package Handle\n",
       "pandas                               pd\n",
       "numpy                                np\n",
       "matplotlib                          mpl\n",
       "matplotlib.pyplot                   plt\n",
       "seaborn                             sns\n",
       "scip.stats.                         sts"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table.dataframe td, table.dataframe th { /* This is for the borders for columns)*/\n",
       "    border: 2px solid black\n",
       "    border-collapse:collapse;\n",
       "    text-align:center;\n",
       "}\n",
       "table.dataframe th {\n",
       "    /*padding:1em 1em;*/\n",
       "    background-color: #000000;\n",
       "    color: #ffffff;\n",
       "    text-align: center;\n",
       "    font-weight: bold;\n",
       "    font-size: 12pt\n",
       "    font-weight: bold;\n",
       "    padding: 0.5em 0.5em;\n",
       "}\n",
       "table.dataframe td:not(:th){\n",
       "    /*border: 1px solid ##e8e8ea;*/\n",
       "    /*background-color: ##e8e8ea;*/\n",
       "    background-color: gainsboro;\n",
       "    text-align: center; \n",
       "    vertical-align: middle;\n",
       "    font-size:10pt;\n",
       "    padding: 0.7em 1em;\n",
       "    /*padding: 0.1em 0.1em;*/\n",
       "}\n",
       "table.dataframe tr:not(:last-child) {\n",
       "    border-bottom: 1px solid gainsboro;\n",
       "}\n",
       "table.dataframe {\n",
       "    /*border-collapse: collapse;*/\n",
       "    background-color: gainsboro; /* This is alternate rows*/\n",
       "    text-align: center;\n",
       "    border: 2px solid black;\n",
       "}\n",
       "table.dataframe th:not(:empty), table.dataframe td{\n",
       "    border-right: 1px solid white;\n",
       "    text-align: center;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import scipy.stats as sts\n",
    "from IPython.display import display\n",
    "import xgboost\n",
    "import sklearn\n",
    "import scipy\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV \n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from scipy.stats import randint, expon\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import xgboost as xbg\n",
    "from xgboost import XGBClassifier\n",
    "import time\n",
    "import re\n",
    "\n",
    "## Styled Dataframe\n",
    "from IPython.display import HTML\n",
    "pd.set_option('display.precision',3)\n",
    "pd.set_option('display.html.border',2)\n",
    "pd.set_option('display.notebook_repr_htm',True)\n",
    "pd.set_option('display.max_columns',None)\n",
    "\n",
    "CSS = \"\"\"\n",
    "table.dataframe td, table.dataframe th { /* This is for the borders for columns)*/\n",
    "    border: 2px solid black\n",
    "    border-collapse:collapse;\n",
    "    text-align:center;\n",
    "}\n",
    "table.dataframe th {\n",
    "    /*padding:1em 1em;*/\n",
    "    background-color: #000000;\n",
    "    color: #ffffff;\n",
    "    text-align: center;\n",
    "    font-weight: bold;\n",
    "    font-size: 12pt\n",
    "    font-weight: bold;\n",
    "    padding: 0.5em 0.5em;\n",
    "}\n",
    "table.dataframe td:not(:th){\n",
    "    /*border: 1px solid ##e8e8ea;*/\n",
    "    /*background-color: ##e8e8ea;*/\n",
    "    background-color: gainsboro;\n",
    "    text-align: center; \n",
    "    vertical-align: middle;\n",
    "    font-size:10pt;\n",
    "    padding: 0.7em 1em;\n",
    "    /*padding: 0.1em 0.1em;*/\n",
    "}\n",
    "table.dataframe tr:not(:last-child) {\n",
    "    border-bottom: 1px solid gainsboro;\n",
    "}\n",
    "table.dataframe {\n",
    "    /*border-collapse: collapse;*/\n",
    "    background-color: gainsboro; /* This is alternate rows*/\n",
    "    text-align: center;\n",
    "    border: 2px solid black;\n",
    "}\n",
    "table.dataframe th:not(:empty), table.dataframe td{\n",
    "    border-right: 1px solid white;\n",
    "    text-align: center;\n",
    "}\n",
    "\"\"\"\n",
    "# HTML('<style>.output {flex-direction: row;}</style>')\n",
    "HTML(f\"<style>{CSS}</style>\")\n",
    "def html_off():\n",
    "    HTML(f\"<style></style>\")\n",
    "def html_on(CSS):\n",
    "    HTML(f'<style>{CSS}</style>')\n",
    "\n",
    "##\n",
    "import_dict = {'pandas':'pd',\n",
    "                 'numpy':'np',\n",
    "                 'matplotlib':'mpl',\n",
    "                 'matplotlib.pyplot':'plt',\n",
    "                 'seaborn':'sns',\n",
    "                 'scip.stats.':'sts'}\n",
    "            \n",
    "# index_range = list(range(1,len(import_dict)))\n",
    "df_imported= pd.DataFrame.from_dict(import_dict,orient='index')\n",
    "df_imported.columns=['Module/Package Handle']\n",
    "display(df_imported)\n",
    "## DataFrame Creation, Inspection, and Exporting\n",
    "def inspect_df(df,n_rows=2):\n",
    "    \"\"\"Displays df.head(),df.info(),df.describe() for dataframe. \n",
    "    Ex: inspect_df(df)\"\"\"\n",
    "    pd.set_option('display.precision',3)\n",
    "    pd.set_option('display.html.border',2)\n",
    "    pd.set_option('display.notebook_repr_htm',True)\n",
    "    display(df.head(n_rows))\n",
    "    display(df.info()), display(df.describe())\n",
    "\n",
    "def list2df(list):#, sort_values='index'):\n",
    "    \"\"\" Take a list where each row becomes a dataframe row and the row[0] contains the columns names.\n",
    "    Ex: list_results = [[\"Test\",\"N\",\"p-val\"]] #... (some sort of analysis performed to produce results)\n",
    "        list_results.append([test_Name,length(data),p])\n",
    "        list2df(list_results)\n",
    "    \"\"\"    \n",
    "    df_list = pd.DataFrame(list[1:],columns=list[0])        \n",
    "    return df_list\n",
    "\n",
    "def drop_cols(df, list_of_strings_or_regexp):#,axis=1):\n",
    "    \"\"\"Take a df, a list of strings or regular expression and recursively \n",
    "    removes all matching column names containing those strings or expressions.\n",
    "    # Example: if the df_in columns are ['price','sqft','sqft_living','sqft15','sqft_living15','floors','bedrooms']\n",
    "    df_out = drop_cols(df_in, ['sqft','bedroom'])\n",
    "    df_out.columns # will output: ['price','floors']\n",
    "    \n",
    "    Parameters:\n",
    "        DF -- input dataframe to remove columns from.\n",
    "        regex_list -- list of string patterns or regexp to remove.\n",
    "    \n",
    "    Returns:\n",
    "        df_dropped -- input df without the dropped columns. \n",
    "    \"\"\"\n",
    "    regex_list=list_of_strings_or_regexp\n",
    "    df_cut = df.copy()\n",
    "    for r in regex_list:\n",
    "        df_cut = df_cut[df_cut.columns.drop(list(df_cut.filter(regex=r)))]\n",
    "        print(f'Removed {r}.')\n",
    "    df_dropped = df_cut\n",
    "    return df_dropped\n",
    "\n",
    "## Dataframes styling\n",
    "\n",
    "# def highlight (helper: hover)\n",
    "def hover(hover_color=\"gold\"):\n",
    "    from IPython.display import HTML\n",
    "    return dict(selector=\"tr:hover\",\n",
    "                props=[(\"background-color\", \"%s\" % hover_color)])\n",
    "def highlight(df,hover_color=\"gold\"):\n",
    "    styles = [\n",
    "        hover(hover_color),\n",
    "        dict(selector=\"th\", props=[(\"font-size\", \"115%\"),\n",
    "                                   (\"text-align\", \"center\")]),\n",
    "        dict(selector=\"caption\", props=[(\"caption-side\", \"bottom\")])\n",
    "    ]\n",
    "    html = (df.style.set_table_styles(styles)\n",
    "              .set_caption(\"Hover to highlight.\"))\n",
    "    return html\n",
    "\n",
    "\n",
    "def color_true_green(val):\n",
    "    \"\"\"Changes text color to green if value is True\n",
    "    Ex: style_df = df.style.applymap(color_true_green)\n",
    "        style_df #to display\"\"\"\n",
    "    color='green' if val==True else 'black'\n",
    "    return f'color: {color}' \n",
    "\n",
    "# Style dataframe for easy visualization\n",
    "def color_scale_columns(df,matplotlib_cmap = \"Greens\",subset=None,):\n",
    "    \"\"\"\n",
    "    Takes a df, any valid matplotlib colormap column names(matplotlib.org/tutorials/colors/colormaps.html) \n",
    "    and returns a dataframe with a gradient colormap applied to column values.\n",
    "    Ex. color_scale_columns(df,\"YlGn\")\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: DataFrame containing columns to style.\n",
    "    subset: Names of columns to color-code.\n",
    "    cmap: Any matplotlib colormap. https://matplotlib.org/tutorials/colors/colormaps.html\n",
    "    Colormap to use instead of default seaborn green.  \n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    df_style : df.style\n",
    "\n",
    "    \"\"\" \n",
    "    from IPython.display import display  \n",
    "    import seaborn as sns\n",
    "    cm = matplotlib_cmap\n",
    "    #     cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "    df_style = df.style.background_gradient(cmap=cm,subset=subset)#,low=results.min(),high=results.max())\n",
    "    # Display styled dataframe\n",
    "#     display(df_style)\n",
    "    return df_style\n",
    "\n",
    "## James' Tree Classifier/Regressor \n",
    "\n",
    "# def tune_params_trees (helpers: performance_r2_mse, performance_roc_auc)\n",
    "def performance_r2_mse(y_true, y_pred):\n",
    "    \"\"\" Calculates and returns the performance score between \n",
    "        true and predicted values based on the metric chosen. \"\"\"\n",
    "    from sklearn.metrics import r2_score\n",
    "    from sklearn.metrics import mean_squared_error as mse\n",
    "    \n",
    "    r2 = r2_score(y_true,y_pred)\n",
    "    MSE = mse(y_true,y_pred)    \n",
    "    return r2, MSE\n",
    "\n",
    "# def performance_roc_auc(X_test,y_test,dtc,verbose=False):\n",
    "def performance_roc_auc(y_true,y_pred):\n",
    "    \"\"\"Tests the results of an already-fit classifer. \n",
    "    Takes y_true (test split), and y_pred (model.predict()), returns the AUC for the roc_curve as a %\"\"\"\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    FP_rate, TP_rate, _ = roc_curve(y_true,y_pred)\n",
    "    roc_auc = auc(FP_rate,TP_rate)\n",
    "    roc_auc_perc = round(roc_auc*100,3)\n",
    "    return roc_auc_perc\n",
    "\n",
    "def tune_params_trees(param_name, param_values, DecisionTreeObject, X,Y,test_size=0.25,perform_metric='r2_mse'):\n",
    "    '''Takes a parame_name (str), param_values (list/array), a DecisionTreeObject, and a perform_metric.\n",
    "    Loops through the param_values and re-fits the model and saves performance metrics. Displays color-mapped dataframe of results and line graph.\n",
    "    \n",
    "    Perform_metric can be 'r2_mse' or 'roc_auc'.\n",
    "    Returns:\n",
    "    - df of results\n",
    "    - styled-df'''\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(test_size=test_size)\n",
    "\n",
    "    # Create results depending on performance metric\n",
    "    if perform_metric=='r2_mse':\n",
    "        results = [['param_name','param_value','r2_test','MSE_test']]\n",
    "        \n",
    "    elif perform_metric=='roc_auc':\n",
    "        results =  [['param_name','param_value','roc_auc_test']]\n",
    "    print(f'Using performance metrics: {perform_metric}')\n",
    "    \n",
    "    # Rename Deicision Tree for looping\n",
    "    dtr_tune =  DecisionTreeObject\n",
    "    \n",
    "    # Loop through each param_value\n",
    "    for value in param_values:\n",
    "\n",
    "        # Set the parameters and fit the model\n",
    "        dtr_tune.set_params(**{param_name:value})\n",
    "        dtr_tune.fit(X_train,y_train)\n",
    "\n",
    "        # Get predicitons and test_performance\n",
    "        y_preds = dtr_tune.predict(X_test)\n",
    "        \n",
    "        # Perform correct performance metric and append results\n",
    "        if perform_metric=='r2_mse':\n",
    "            \n",
    "            r2_test, mse_test = performance_r2_mse(y_test,y_preds)\n",
    "            results.append([param_name,value,r2_test,mse_test])\n",
    "        \n",
    "        elif perform_metric=='roc_auc':\n",
    "            \n",
    "            roc_auc_test = performance_roc_auc(y_test,y_preds)\n",
    "            results.append([param_name,value,roc_auc_test])\n",
    "     \n",
    "\n",
    "    # Convert results to dataframe, set index\n",
    "    df_results = list2df(results)\n",
    "    df_results.set_index('param_value',inplace=True)\n",
    "\n",
    "\n",
    "    # Plot the values in results\n",
    "    df_results.plot(subplots=True,sharex=True)\n",
    "\n",
    "    # Style dataframe for easy visualization\n",
    "    import seaborn as sns\n",
    "    cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "    df_style = df_results.style.background_gradient(cmap=cm,subset=['r2_test','MSE_test'])#,low=results.min(),high=results.max())\n",
    "    # Display styled dataframe\n",
    "    from IPython.display import display  \n",
    "    display(df_style)\n",
    "    \n",
    "    return df_results\n",
    "\n",
    "\n",
    "# Display graphviz tree\n",
    "def viz_tree(tree_object):\n",
    "    '''Takes a Sklearn Decision Tree and returns a png image using graph_viz and pydotplus.'''\n",
    "    # Visualize the decision tree using graph viz library \n",
    "    from sklearn.externals.six import StringIO  \n",
    "    from IPython.display import Image  \n",
    "    from sklearn.tree import export_graphviz\n",
    "    import pydotplus\n",
    "    dot_data = StringIO()\n",
    "    export_graphviz(tree_object, out_file=dot_data, filled=True, rounded=True,special_characters=True)\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "    tree_viz = Image(graph.create_png())\n",
    "    return tree_viz\n",
    "\n",
    "# EDA / Plotting Functions\n",
    "def multiplot(df):\n",
    "    \"\"\"Plots results from df.corr() in a correlation heat map for multicollinearity.\n",
    "    Returns fig, ax objects\"\"\"\n",
    "    import seaborn as sns\n",
    "    sns.set(style=\"white\")\n",
    "    from string import ascii_letters\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "    # Compute the correlation matrix\n",
    "    corr = df.corr()\n",
    "\n",
    "    # Generate a mask for the upper triangle\n",
    "    mask = np.zeros_like(corr, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    # Set up the matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=(16, 16))\n",
    "\n",
    "    # Generate a custom diverging colormap\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(corr, mask=mask, annot=True, cmap=cmap, center=0,\n",
    "                \n",
    "    square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "    return f, ax\n",
    "\n",
    "# Plots histogram and scatter (vs price) side by side\n",
    "def plot_hist_scat(df, target='index'):\n",
    "    \"\"\"Plots seaborne distplots and regplots for columns im datamframe vs target.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame.describe() columns will be used. \n",
    "    target = name of column containing target variable.assume first coluumn. \n",
    "    \n",
    "    Returns:\n",
    "    Figures for each column vs target with 2 subplots.\n",
    "   \"\"\"\n",
    "    import matplotlib.ticker as mtick\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    with plt.style.context(('dark_background')):\n",
    "        ###  DEFINE AESTHETIC CUSTOMIZATIONS  -------------------------------##\n",
    "        figsize=(14,10)\n",
    "\n",
    "        # Axis Label fonts\n",
    "        fontTitle = {'fontsize': 16,\n",
    "                   'fontweight': 'bold',\n",
    "                    'fontfamily':'serif'}\n",
    "\n",
    "        fontAxis = {'fontsize': 14,\n",
    "                   'fontweight': 'bold',\n",
    "                    'fontfamily':'serif'}\n",
    "\n",
    "        fontTicks = {'fontsize': 12,\n",
    "                   'fontweight':'bold',\n",
    "                    'fontfamily':'serif'}\n",
    "\n",
    "        # Formatting dollar sign labels\n",
    "        # fmtPrice = '${x:,.0f}'\n",
    "        # tickPrice = mtick.StrMethodFormatter(fmtPrice)\n",
    "\n",
    "\n",
    "        ###  PLOTTING ----------------------------- ------------------------ ##\n",
    "\n",
    "        # Loop through dataframe to plot\n",
    "        for column in df.describe():\n",
    "\n",
    "            # Create figure with subplots for current column\n",
    "            fig, ax = plt.subplots(figsize=figsize, ncols=2, nrows=2)\n",
    "\n",
    "            ##  SUBPLOT 1 --------------------------------------------------##\n",
    "            i,j = 0,0\n",
    "            ax[i,j].set_title(column.capitalize(),fontdict=fontTitle)\n",
    "\n",
    "            # Define graphing keyword dictionaries for distplot (Subplot 1)\n",
    "            hist_kws = {\"linewidth\": 1, \"alpha\": 1, \"color\": 'steelblue','edgecolor':'w','hatch':'\\\\'}\n",
    "            kde_kws = {\"color\": \"white\", \"linewidth\": 3, \"label\": \"KDE\",'alpha':0.7}\n",
    "\n",
    "            # Plot distplot on ax[i,j] using hist_kws and kde_kws\n",
    "            sns.distplot(df[column], norm_hist=True, kde=True,\n",
    "                         hist_kws = hist_kws, kde_kws = kde_kws,\n",
    "                         label=column+' histogram', ax=ax[i,j])\n",
    "\n",
    "\n",
    "            # Set x axis label\n",
    "            ax[i,j].set_xlabel(column.title(),fontdict=fontAxis)\n",
    "\n",
    "            # Get x-ticks, rotate labels, and return\n",
    "            xticklab1 = ax[i,j].get_xticklabels(which = 'both')\n",
    "            ax[i,j].set_xticklabels(labels=xticklab1, fontdict=fontTicks, rotation=0)\n",
    "            ax[i,j].xaxis.set_major_formatter(mtick.ScalarFormatter())\n",
    "\n",
    "\n",
    "            # Set y-label \n",
    "            ax[i,j].set_ylabel('Density',fontdict=fontAxis)\n",
    "            yticklab1=ax[i,j].get_yticklabels(which='both')\n",
    "            ax[i,j].set_yticklabels(labels=yticklab1,fontdict=fontTicks)\n",
    "            ax[i,j].yaxis.set_major_formatter(mtick.ScalarFormatter())\n",
    "\n",
    "\n",
    "            # Set y-grid\n",
    "            ax[i, j].set_axisbelow(True)\n",
    "            ax[i, j].grid(axis='y',ls='--')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            ##  SUBPLOT 2-------------------------------------------------- ##\n",
    "            i,j = 0,1\n",
    "            ax[i,j].set_title(column.capitalize(),fontdict=fontTitle)\n",
    "\n",
    "            # Define the kwd dictionaries for scatter and regression line (subplot 2)\n",
    "            line_kws={\"color\":\"white\",\"alpha\":0.5,\"lw\":3,\"ls\":\":\"}\n",
    "            scatter_kws={'s': 2, 'alpha': 0.8,'marker':'.','color':'steelblue'}\n",
    "\n",
    "            # Plot regplot on ax[i,j] using line_kws and scatter_kws\n",
    "            sns.regplot(df[column], df[target], \n",
    "                        line_kws = line_kws,\n",
    "                        scatter_kws = scatter_kws,\n",
    "                        ax=ax[i,j])\n",
    "\n",
    "            # Set x-axis label\n",
    "            ax[i,j].set_xlabel(column.title(),fontdict=fontAxis)\n",
    "\n",
    "             # Get x ticks, rotate labels, and return\n",
    "            xticklab2=ax[i,j].get_xticklabels(which='both')\n",
    "            ax[i,j].set_xticklabels(labels=xticklab2,fontdict=fontTicks, rotation=0)\n",
    "            ax[i,j].xaxis.set_major_formatter(mtick.ScalarFormatter())\n",
    "\n",
    "            # Set  y-axis label\n",
    "            ax[i,j].set_ylabel(target.title(),fontdict=fontAxis)\n",
    "\n",
    "            # Get, set, and format y-axis Price labels\n",
    "            yticklab = ax[i,j].get_yticklabels()\n",
    "            ax[i,j].set_yticklabels(yticklab,fontdict=fontTicks)\n",
    "            ax[i,j].yaxis.set_major_formatter(mtick.ScalarFormatter())\n",
    "\n",
    "            # Set y-grid\n",
    "            ax[i, j].set_axisbelow(True)\n",
    "            ax[i, j].grid(axis='y',ls='--')       \n",
    "\n",
    "            ## ---------- Final layout adjustments ----------- ##\n",
    "            # Deleted unused subplots \n",
    "            fig.delaxes(ax[1,1])\n",
    "            fig.delaxes(ax[1,0])\n",
    "\n",
    "            # Optimizing spatial layout\n",
    "            fig.tight_layout()\n",
    "            # figtitle=column+'_dist_regr_plots.png'\n",
    "            # plt.savefig(figtitle)\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "## Mike's Plotting Functions\n",
    "def draw_violinplot(x , y, hue=None, data=None, title=None,\n",
    "                    ticklabels=None, leg_label=None):\n",
    "    \n",
    "    '''Plots a violin plot with horizontal mean line, inner stick lines\n",
    "    y must be arraylike in order to plot mean line. x can be label in data'''\n",
    "\n",
    "    \n",
    "    fig,ax = plt.subplots(figsize=(12,10))\n",
    "\n",
    "    sns.violinplot(x, y, hue=hue,\n",
    "                   data = data,\n",
    "                   cut=2,\n",
    "                   split=True, \n",
    "                   scale='count',\n",
    "                   scale_hue=True,\n",
    "                   saturation=.7,\n",
    "                   alpha=.9, \n",
    "                   bw=.25,\n",
    "                   palette='Dark2',\n",
    "                   inner='stick'\n",
    "                  ).set_title(title)\n",
    "    \n",
    "    ax.set(xlabel= x.name.title(),\n",
    "          ylabel= y.name.title(),\n",
    "           xticklabels=ticklabels)\n",
    "    \n",
    "    ax.axhline( y.mean(),\n",
    "               label='Total Mean',\n",
    "               ls=':',\n",
    "               alpha=.2, \n",
    "               color='xkcd:yellow')\n",
    "    \n",
    "    ax.legend().set_title(leg_label)\n",
    "\n",
    "    plt.show()\n",
    "    return fig, ax\n",
    "\n",
    "## Finding outliers and statistics\n",
    "# Tukey's method using IQR to eliminate \n",
    "def detect_outliers(df, n, features):\n",
    "    \"\"\"Uses Tukey's method to return outer of interquartile ranges to return indices if outliers in a dataframe.\n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame containing columns of features\n",
    "    n: default is 0, multiple outlier cutoff  \n",
    "    \n",
    "    Returns:\n",
    "    Index of outliers for .loc\n",
    "    \n",
    "    Examples:\n",
    "    Outliers_to_drop = detect_outliers(data,2,[\"col1\",\"col2\"]) Returning value\n",
    "    df.loc[Outliers_to_drop] # Show the outliers rows\n",
    "    data= data.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)\n",
    "\"\"\"\n",
    "\n",
    "# Drop outliers    \n",
    "\n",
    "    outlier_indices = []\n",
    "    # iterate over features(columns)\n",
    "    for col in features:\n",
    "        \n",
    "        # 1st quartile (25%)\n",
    "        Q1 = np.percentile(df[col], 25)\n",
    "        # 3rd quartile (75%)\n",
    "        Q3 = np.percentile(df[col],75)\n",
    "        \n",
    "        # Interquartile range (IQR)\n",
    "        IQR = Q3 - Q1\n",
    "        # outlier step\n",
    "        outlier_step = 1.5 * IQR\n",
    "        \n",
    "        # Determine a list of indices of outliers for feature col\n",
    "        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n",
    "        \n",
    "        # append the found outlier indices for col to the list of outlier indices \n",
    "        outlier_indices.extend(outlier_list_col)\n",
    "        \n",
    "        # select observations containing more than 2 outliers\n",
    "        from collections import Counter\n",
    "        outlier_indices = Counter(outlier_indices)        \n",
    "        multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n",
    "    return multiple_outliers \n",
    "\n",
    "\n",
    "def find_outliers(column):\n",
    "    quartile_1, quartile_3 = np.percentile(column, [25, 75])\n",
    "    IQR = quartile_3 - quartile_1\n",
    "    low_outlier = quartile_1 - (IQR * 1.5)\n",
    "    high_outlier = quartile_3 + (IQR * 1.5)    \n",
    "    outlier_index = column[(column < low_outlier) | (column > high_outlier)].index\n",
    "    return outlier_index\n",
    "\n",
    "# describe_outliers -- calls find_outliers\n",
    "def describe_outliers(df):\n",
    "    \"\"\" Returns a new_df of outliers, and % outliers each col using detect_outliers.\n",
    "    \"\"\"\n",
    "    out_count = 0\n",
    "    new_df = pd.DataFrame(columns=['total_outliers', 'percent_total'])\n",
    "    for col in df.columns:\n",
    "        outies = find_outliers(df[col])\n",
    "        out_count += len(outies) \n",
    "        new_df.loc[col] = [len(outies), round((len(outies)/len(df.index))*100, 2)]\n",
    "    new_df.loc['grand_total'] = [sum(new_df['total_outliers']), sum(new_df['percent_total'])]\n",
    "    return new_df\n",
    "\n",
    "\n",
    "#### Cohen's d\n",
    "def Cohen_d(group1, group2):\n",
    "    '''Compute Cohen's d.\n",
    "    # group1: Series or NumPy array\n",
    "    # group2: Series or NumPy array\n",
    "    # returns a floating point number \n",
    "    '''\n",
    "    diff = group1.mean() - group2.mean()\n",
    "\n",
    "    n1, n2 = len(group1), len(group2)\n",
    "    var1 = group1.var()\n",
    "    var2 = group2.var()\n",
    "\n",
    "    # Calculate the pooled threshold as shown earlier\n",
    "    pooled_var = (n1 * var1 + n2 * var2) / (n1 + n2)\n",
    "    \n",
    "    # Calculate Cohen's d statistic\n",
    "    d = diff / np.sqrt(pooled_var)\n",
    "    \n",
    "    return d\n",
    "\n",
    "\n",
    "\n",
    "#####\n",
    "def subplot_imshow(images, num_images,num_rows, num_cols, figsize=(20,15)):\n",
    "    '''\n",
    "    Takes image data and plots a figure with subplots for as many images as given.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    images: str, data in form data.images ie. olivetti images\n",
    "    num_images: int, number of images\n",
    "    num_rows: int, number of rows to plot.\n",
    "    num_cols: int, number of columns to plot\n",
    "    figize: tuple, size of figure default=(20,15)\n",
    "    \n",
    "    returns:  figure with as many subplots as images given\n",
    "    '''\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    for i in range(num_images):\n",
    "        ax = fig.add_subplot(num_rows,num_cols, i+1, xticks=[], yticks=[])\n",
    "        ax.imshow(images[i],cmap=plt.gray)\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "    return fig, ax\n",
    "#####\n",
    "###########\n",
    "def plot_wide_kde_thin_bars(series1,sname1, series2, sname2):\n",
    "    '''Plot series1 and series 2 on wide kde plot with small mean+sem bar plot.'''\n",
    "    \n",
    "    ## ADDING add_gridspec usage\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from scipy.stats import sem\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib as mpl\n",
    "    import matplotlib.ticker as ticker\n",
    "\n",
    "    import seaborn as sns\n",
    "\n",
    "    from matplotlib import rcParams\n",
    "    from matplotlib import rc\n",
    "    rcParams['font.family'] = 'serif'\n",
    "\n",
    "    # Plot distributions of discounted vs full price groups\n",
    "    plt.style.use('default')\n",
    "    # with plt.style.context(('tableau-colorblind10')):\n",
    "    with plt.style.context(('seaborn-notebook')):\n",
    "\n",
    "        ## ----------- DEFINE AESTHETIC CUSTOMIZATIONS ----------- ##\n",
    "       # Axis Label fonts\n",
    "        fontSuptitle ={'fontsize': 22,\n",
    "                   'fontweight': 'bold',\n",
    "                    'fontfamily':'serif'}\n",
    "\n",
    "        fontTitle = {'fontsize': 10,\n",
    "                   'fontweight': 'medium',\n",
    "                    'fontfamily':'serif'}\n",
    "\n",
    "        fontAxis = {'fontsize': 10,\n",
    "                   'fontweight': 'medium',\n",
    "                    'fontfamily':'serif'}\n",
    "\n",
    "        fontTicks = {'fontsize': 8,\n",
    "                   'fontweight':'medium', \n",
    "                    'fontfamily':'serif'}\n",
    "\n",
    "\n",
    "        ## --------- CREATE FIG BASED ON GRIDSPEC --------- ##\n",
    "        \n",
    "        plt.suptitle('Quantity of Units Sold', fontdict = fontSuptitle)\n",
    "\n",
    "        # Create fig object and declare figsize\n",
    "        fig = plt.figure(constrained_layout=True, figsize=(8,3))\n",
    "        \n",
    "        \n",
    "        # Define gridspec to create grid coordinates             \n",
    "        gs = fig.add_gridspec(nrows=1,ncols=10)\n",
    "\n",
    "        # Assign grid space to ax with add_subplot\n",
    "        ax0 = fig.add_subplot(gs[0,0:7])\n",
    "        ax1 = fig.add_subplot(gs[0,7:10])\n",
    "        \n",
    "        #Combine into 1 list\n",
    "        ax = [ax0,ax1]\n",
    "        \n",
    "        ### ------------------  SUBPLOT 1  ------------------ ###\n",
    "\n",
    "        ## --------- Defining series1 and 2 for subplot 1------- ##\n",
    "        ax[0].set_title('Histogram + KDE',fontdict=fontTitle)\n",
    "\n",
    "        # Group 1: data, label, hist_kws and kde_kws\n",
    "        plotS1 = {'data': series1, 'label': sname1.title(),\n",
    "\n",
    "                   'hist_kws' :\n",
    "                    {'edgecolor': 'black', 'color':'darkgray','alpha': 0.8, 'lw':0.5},\n",
    "\n",
    "                   'kde_kws':\n",
    "                    {'color':'gray', 'linestyle': '--', 'linewidth':2,\n",
    "                     'label':'kde'}}\n",
    "\n",
    "        # Group 2: data, label, hist_kws and kde_kws\n",
    "        plotS2 = {'data': series2,\n",
    "                    'label': sname2.title(), \n",
    "\n",
    "                    'hist_kws' :\n",
    "                    {'edgecolor': 'black','color':'green','alpha':0.8 ,'lw':0.5},\n",
    "\n",
    "\n",
    "                    'kde_kws':\n",
    "                    {'color':'darkgreen','linestyle':':','linewidth':3,'label':'kde'}}\n",
    "        \n",
    "        # plot group 1\n",
    "        sns.distplot(plotS1['data'], label=plotS1['label'],\n",
    "                   \n",
    "                     hist_kws = plotS1['hist_kws'], kde_kws = plotS1['kde_kws'],\n",
    "                     \n",
    "                     ax=ax[0])   \n",
    "      \n",
    "\n",
    "        # plot group 2\n",
    "        sns.distplot(plotS2['data'], label=plotS2['label'],\n",
    "                     \n",
    "                     hist_kws=plotS2['hist_kws'], kde_kws = plotS2['kde_kws'],\n",
    "                     \n",
    "                     ax=ax[0])\n",
    "\n",
    "\n",
    "        ax[0].set_xlabel(series1.name, fontdict=fontAxis)\n",
    "        ax[0].set_ylabel('Kernel Density Estimation',fontdict=fontAxis)\n",
    "\n",
    "        ax[0].tick_params(axis='both',labelsize=fontTicks['fontsize'])   \n",
    "        ax[0].legend()\n",
    "\n",
    "\n",
    "        ### ------------------  SUBPLOT 2  ------------------ ###\n",
    "        \n",
    "        # Import scipy for error bars\n",
    "        from scipy.stats import sem\n",
    "    \n",
    "        # Declare x y group labels(x) and bar heights(y)\n",
    "        x = [plotS1['label'], plotS2['label']]\n",
    "        y = [np.mean(plotS1['data']), np.mean(plotS2['data'])]\n",
    "\n",
    "        yerr = [sem(plotS1['data']), sem(plotS2['data'])]\n",
    "        err_kws = {'ecolor':'black','capsize':5,'capthick':1,'elinewidth':1}\n",
    "\n",
    "        # Create the bar plot\n",
    "        ax[1].bar(x,y,align='center', edgecolor='black', yerr=yerr,error_kw=err_kws,width=0.6)\n",
    "\n",
    "        \n",
    "        # Customize subplot 2\n",
    "        ax[1].set_title('Average Quantities Sold',fontdict=fontTitle)\n",
    "        ax[1].set_ylabel('Mean +/- SEM ',fontdict=fontAxis)\n",
    "        ax[1].set_xlabel('')\n",
    "        \n",
    "        ax[1].tick_params(axis=y,labelsize=fontTicks['fontsize'])\n",
    "        ax[1].tick_params(axis=x,labelsize=fontTicks['fontsize']) \n",
    "\n",
    "        ax1=ax[1]\n",
    "        test = ax1.get_xticklabels()\n",
    "        labels = [x.get_text() for x in test]\n",
    "        ax1.set_xticklabels([plotS1['label'],plotS2['label']], rotation=45,ha='center')\n",
    "        \n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        return fig,ax\n",
    "\n",
    "\n",
    "def scale_data(data, method='minmax', log=False):\n",
    "    \n",
    "    \"\"\"Takes df or Series, scales it using desired method and returns scaled df.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    data : pd.Series or pd.DataFrame\n",
    "        entire dataframe of series to be scaled\n",
    "    method : str\n",
    "        The method for scaling to be implemented(default is 'minmax').\n",
    "        Other options are 'standard' or 'robust'.\n",
    "    log : bool, optional\n",
    "        Takes log of data if set to True(deafault is False).\n",
    "        \n",
    "    Returns\n",
    "    --------\n",
    "    pd.DataFrame of scaled data.\n",
    "    \"\"\"\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "    \n",
    "    scale = np.array(data)\n",
    "    \n",
    "    # reshape if needed\n",
    "    if len(scale.shape) == 1:\n",
    "        scale = scale.reshape(-1,1)\n",
    "        \n",
    "    # takes log if log=True  \n",
    "    if log == True:\n",
    "        scale = np.log(scale)\n",
    "        \n",
    "        \n",
    "    # creates chosen scaler instance\n",
    "    if method == 'robust':\n",
    "        scaler = RobustScaler()\n",
    "        \n",
    "    elif method == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "        \n",
    "    else:\n",
    "        scaler = MinMaxScaler()   \n",
    "    scaled = scaler.fit_transform(scale)\n",
    "    \n",
    "    \n",
    "    # reshape and create output DataFrame\n",
    "    if  scaled.shape[1] > 1:\n",
    "        df_scaled = pd.DataFrame(scaled, index=data.index, columns=data.columns)\n",
    "        \n",
    "    else:\n",
    "        scaled = np.squeeze(scaled)\n",
    "        scaled = pd.Series(scaled, name=data.name) \n",
    "        df_scaled = pd.DataFrame(scaled, index=data.index)\n",
    "        \n",
    "    return df_scaled\n",
    "\n",
    "\n",
    "## Mike's modeling:\n",
    "# import xgboost\n",
    "# import sklearn\n",
    "# import scipy\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.linear_model import LogisticRegression, LogisticRegressionCV \n",
    "# from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "# from scipy.stats import randint, expon\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import VotingClassifier\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# import xgboost as xbg\n",
    "# from xgboost import XGBClassifier\n",
    "# import time\n",
    "# import re\n",
    "\n",
    "def select_pca(features, n_components_list):\n",
    "    \n",
    "    '''\n",
    "    Takes features and list of n_components to run PCA on\n",
    "    \n",
    "    Params:\n",
    "    ----------\n",
    "    features: pd.Dataframe\n",
    "    n_components: list of ints to pass to PCA n_component parameter\n",
    "    \n",
    "    returns:\n",
    "    ----------\n",
    "    pd.DataFrame, displays number of components and their respective \n",
    "    explained variance ratio\n",
    "    '''\n",
    "    \n",
    "    from JMI_MVM import list2df\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    # Create list to store results in\n",
    "    results = [['Model','n_components', 'Explained_Variance_ratio_']]\n",
    "    \n",
    "    # Loop through list of components to do PCA on\n",
    "    for n in n_components_list:\n",
    "        \n",
    "        # Creat instance of PCA class\n",
    "        pca = PCA(n_components=n)\n",
    "        pca.fit_transform(features)\n",
    "        \n",
    "        # Create list of n_component and Explained Variance\n",
    "        component_variance = ['PCA',n, np.sum(pca.explained_variance_ratio_)]\n",
    "        \n",
    "        # Append list results list\n",
    "        results.append(component_variance)\n",
    "        \n",
    "        # Use list2df to display results in DataFrame\n",
    "    return list2df(results)\n",
    "\n",
    "\n",
    "\n",
    "def train_test_dict(X, y, test_size=.25, random_state=42):\n",
    "    \n",
    "    \"\"\"\n",
    "    Splits data into train/test sets and returns diction with each variable its own key and value.\n",
    "    \"\"\"\n",
    "\n",
    "    train_test = {}\n",
    "    X_train, y_train, X_test, y_test = train_test_split(X, y, test_size, random_state)\n",
    "    train_test['X_train'] = X_train\n",
    "    train_test['y_train'] = y_train\n",
    "    train_test['X_test'] = X_test\n",
    "    train_test['y_test'] = y_test\n",
    "\n",
    "    return train_test\n",
    "\n",
    "def make_estimators_dict():\n",
    "    \n",
    "    \"\"\"\n",
    "    Instantiates models as first step for creating pipelines.\n",
    "    \n",
    "    \"\"\"\n",
    "    # instantiate classifier objects\n",
    "    xgb = XGBClassifier()\n",
    "    svc = SVC()\n",
    "    lr = LogisticRegression()\n",
    "    gb = GradientBoostingClassifier()\n",
    "    rf = RandomForestClassifier()\n",
    "    dt = DecisionTreeClassifier()\n",
    "    ab = AdaBoostClassifier()\n",
    "\n",
    "    estimators = {\n",
    "        'xgb': xgb,\n",
    "        'SVC': svc,\n",
    "        'Logisic Regression': lr,\n",
    "        'GradientBoosting': gb,\n",
    "        'Random Forest': rf,\n",
    "        'Decision Tree': dt,\n",
    "        'AdaBoost': ab\n",
    "    }\n",
    "    return estimators\n",
    "\n",
    "\n",
    "def make_pipes(estimators_dict, scaler=StandardScaler, n_components='mle', random_state=42):\n",
    "\n",
    "    \"\"\"\n",
    "    Makes pipelines for given models, outputs dictionaries with keys as names and pipeline objects as values.\n",
    "    \n",
    "    Parameters:\n",
    "    ---------------\n",
    "    estimators: dict,\n",
    "            dictionary with name (str) as key and estimator objects as values.\n",
    "    scaler: sklearn.preprocessing instave\n",
    "    \"\"\"\n",
    "\n",
    "    # Create dictionary to store pipelines\n",
    "    pipe_dict = {}\n",
    "\n",
    "    # Instantiate piplines for each model\n",
    "    for k, v in estimators_dict.items():\n",
    "        pipe = Pipeline([('scaler', scaler()),\n",
    "                        ('pca', PCA(n_components=n_components,random_state=random_state)),\n",
    "                        ('clf', v(random_state=random_state))])\n",
    "        # append to dictionary\n",
    "        pipe_dict[k] = pipe\n",
    "    \n",
    "    return pipe_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fit_pipes(pipes_dict, train_test, predict=True, verbose=True, score='accuracy'):\n",
    "\n",
    "    \"\"\"\n",
    "    Fits piplines to training data, if predict=True, it displays a dataframe of scores.\n",
    "    score can be either 'accuracy' or 'roc_auc'. rco_auc_score should be used with binary classification.\n",
    "    \n",
    "     \"\"\"\n",
    "\n",
    "    fit_pipes = {}\n",
    "    score_display = [['Estimator', f'Test {score}']]\n",
    "    \n",
    "    # Assert test/train sets are approriate types\n",
    "    if type(train_test) == dict:\n",
    "        X = train_test['X_train']\n",
    "        y = train_test['y_train']\n",
    "        X_test = train_test['X_test']\n",
    "        y_test = train_test['y_test']\n",
    "\n",
    "    elif type(train_test) == list:\n",
    "        X = train_test[0]\n",
    "        y = train_test[1]\n",
    "        X_test = train_test[2]\n",
    "        y_test = train_test[3]\n",
    "\n",
    "    else:\n",
    "        raise ValueError('train_test must be either list or dictionary')\n",
    "        \n",
    "    # Implement timer    \n",
    "    start = time.time()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'fitting {len(pipes_dict)} models')\n",
    "\n",
    "    # Fit pipes, predict if True\n",
    "    for name, pipe in pipes_dict.items():\n",
    "\n",
    "        fit_pipe = pipe.fit(X, y)\n",
    "        fit_pipes['name'] = fit_pipe\n",
    "        \n",
    "        # Get accuracy or roc_auc score ,append to display list\n",
    "        if predict:\n",
    "            print(f'\\nscoring {name} model')\n",
    "\n",
    "            if score == 'accuracy':\n",
    "                score_display.append(name, fit_pipe.score(X_test, y_test))\n",
    "\n",
    "            elif score == 'roc_auc':\n",
    "                score_display.append(name, roc_auc_score(y_test,fit_pipe.decision_function(X_test)))\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f\"score expected 'accuracy' of 'roc_auc', was given {score}\")\n",
    "    # End timer\n",
    "    stop = time.time()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'\\nTime to fit all pipeline:{(stop-start)/60} minutes')\n",
    "\n",
    "    # display results dataframe if prediction and verbosity\n",
    "    if predict:\n",
    "        display(list2df(score_display))\n",
    "    \n",
    "    return fit_pipes\n",
    "config_dict = {\n",
    "    sklearn.linear_model.LogisticRegressionCV:[{\n",
    "\n",
    "        }],\n",
    "        sklearn.linear_model.LogisticRegression:[{\n",
    "            'clf__penalty':['l1'],\n",
    "            'clf__C':[0.1, 1, 10, 15 ],\n",
    "            'clf__tol':[1e-5, 1e-4, 1e-3],\n",
    "            'clf__solver':['liblinear', 'newton-cg'],\n",
    "            'clf__n_jobs':[-1]\n",
    "            }, {\n",
    "            'clf__penalty':['l2'],\n",
    "            'clf__C':[0.1, 1, 10, 15 ],\n",
    "            'clf__tol':[1e-5, 1e-4, 1e-3],\n",
    "            'clf__solver':['lbfgs', 'sag'],\n",
    "            'clf__n_jobs':[-1]\n",
    "            }], \n",
    "            sklearn.ensemble.RandomForestClassifier:[{\n",
    "                'clf__n_estimators':[10, 50, 100], \n",
    "                'clf__criterion':['gini', 'entropy'],\n",
    "                'clf__max_depth':[4, 6, 10], \n",
    "                'clf__min_samples_leaf':[0.1, 1, 5, 15],\n",
    "                'clf__min_samples_split':[0.05 ,0.1, 0.2],\n",
    "                'clf__n_jobs':[-1]\n",
    "                }],\n",
    "                sklearn.svm.SVC:[{\n",
    "                    'clf__C': [0.1, 1, 10], \n",
    "                    'clf__kernel': ['linear']\n",
    "                    },{\n",
    "                    'clf__C': [1, 10], \n",
    "                    'clf__gamma': [0.001, 0.01], \n",
    "                    'clf__kernel': ['rbf']\n",
    "                    }],\n",
    "                    sklearn.ensemble.GradientBoostingClassifier:[{\n",
    "                        'clf__loss':['deviance'], \n",
    "                        'clf__learning_rate': [0.1, 0.5, 1.0],\n",
    "                        'clf__n_estimators': [50, 100, 150]\n",
    "                        }], \n",
    "                        xgboost.sklearn.XGBClassifier:[{\n",
    "                            'clf__learning_rate':[.001, .01],\n",
    "                            'clf__n_estimators': [1000,  100],\n",
    "                            'clf__max_depth': [3, 5]\n",
    "                            }]\n",
    "    }\n",
    "    \n",
    "random_config_dict = {\n",
    "    sklearn.ensemble.RandomForestClassifier:{\n",
    "        'clf__n_estimators': [100 ,500, 1000],\n",
    "        'clf__criterion': ['gini', 'entropy'],\n",
    "        'clf__max_depth': randint(1,100),\n",
    "        'clf__max_features': randint(1,100),\n",
    "        'clf__min_samples_leaf': randint(1, 100),\n",
    "        'clf__min_samples_split': randint(2, 10),\n",
    "        'clf__n_jobs':[-1]\n",
    "        }, \n",
    "        xgboost.sklearn.XGBClassifier:{\n",
    "            'clf__silent': [False],\n",
    "            'clf__max_depth': [6, 10, 15, 20],\n",
    "            'clf__learning_rate': [0.001, 0.01, 0.1, 0.2, 0,3],\n",
    "            'clf__subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "            'clf__colsample_bytree': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "            'clf__colsample_bylevel': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "            'clf__min_child_weight': [0.5, 1.0, 3.0, 5.0, 7.0, 10.0],\n",
    "            'clf__gamma': [0, 0.25, 0.5, 1.0],\n",
    "            'clf__reg_lambda': [0.1, 1.0, 5.0, 10.0, 50.0, 100.0],\n",
    "            'clf__n_estimators': [100]\n",
    "            },\n",
    "            sklearn.svm.SVC:{\n",
    "                'clf__C': scipy.stats.expon(scale=100), \n",
    "                'clf__gamma': scipy.stats.expon(scale=.1),\n",
    "                'clf__kernel': ['linear','rbf'], \n",
    "                'clf__class_weight':['balanced', None]\n",
    "                }\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def pipe_search(estimator, params, X_train, y_train, X_test, y_test, n_components='mle',\n",
    "                scaler=StandardScaler(), random_state=42, cv=3, verbose=2, n_jobs=-1):\n",
    "\n",
    "    \"\"\"\n",
    "    Fits pipeline and performs a grid search with cross validation using with given estimator\n",
    "    and parameters.\n",
    "    \n",
    "    Parameters:\n",
    "    --------------\n",
    "    estimator: estimator object,\n",
    "            This is assumed to implement the scikit-learn estimator interface. \n",
    "            Ex. sklearn.svm.SVC\n",
    "    params: dict, list of dicts,\n",
    "            Dictionary with parameters names (string) as keys and lists of parameter \n",
    "            settings to try as values, or a list of such dictionaries, in which case\n",
    "            the grids spanned by each dictionary in the list are explored.This enables\n",
    "            searching over any sequence of parameter settings.\n",
    "            MUST BE IN FORM: 'clf__param_'. ex. 'clf__C':[1, 10, 100]\n",
    "    X_train, y_train, X_test, y_test: \n",
    "            training and testing data to fit, test to model\n",
    "    n_components: int, float, None or str. default='mle'\n",
    "            Number of components to keep. if n_components is not set all components are kept.\n",
    "            If n_components == 'mle'  Minka’s MLE is used to guess the dimension. For PCA.\n",
    "    random_state: int, RandomState instance or None, optional, default=42\n",
    "            Pseudo random number generator state used for random uniform sampling from lists of \n",
    "            possible values instead of scipy.stats distributions. If int, random_state is the \n",
    "            seed used by the random number generator; If RandomState instance, random_state \n",
    "            is the random number generator; If None, the random number generator is the \n",
    "            RandomState instance used by np.random.\n",
    "    cv:  int, cross-validation generator or an iterable, optional\n",
    "            Determines the cross-validation splitting strategy. Possible inputs for cv are:\n",
    "                None, to use the default 3-fold cross validation,\n",
    "                integer, to specify the number of folds in a (Stratified)KFold,\n",
    "                CV splitter,\n",
    "                An iterable yielding (train, test) splits as arrays of indices.\n",
    "    verbose : int,\n",
    "            Controls the verbosity: the higher, the more messages.\n",
    "    n_jobs : int or None, optional (default = -1)\n",
    "            Number of jobs to run in parallel. None means 1 unless in a joblib.parallel_backend \n",
    "            context. -1 means using all processors. See Glossary for more details.\n",
    "\n",
    "    Returns:\n",
    "    ------------\n",
    "        dictionary:\n",
    "                keys are: 'test_score' , 'best_accuracy' (training validation score),\n",
    "                'best_params', 'best_estimator', 'results'\n",
    "    \"\"\"\n",
    "    # create dictionary to store results.\n",
    "    results = {}\n",
    "    # Instantiate pipeline object.\n",
    "    pipe = Pipeline([('scaler', scaler),\n",
    "                        ('pca', PCA(n_components=n_components,random_state=random_state)),\n",
    "                        ('clf', estimator(random_state=random_state))])\n",
    "\n",
    "    # start timer and fit pipeline.                        \n",
    "    start = time.time()\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    # Instantiate and fit gridsearch object.\n",
    "    grid = GridSearchCV(estimator = pipe,\n",
    "        param_grid = params,\n",
    "        scoring = 'accuracy',\n",
    "        cv = cv, verbose = verbose, n_jobs=n_jobs, return_train_score = True)\n",
    "\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    # Store results in dictionary.\n",
    "    results['test_score'] = grid.score(X_test, y_test)\n",
    "    results['best_accuracy'] = grid.best_score_\n",
    "    results['best_params'] = grid.best_params_\n",
    "    results['best_estimator'] = grid.best_estimator_\n",
    "    results['results'] = grid.cv_results_\n",
    "\n",
    "    # End timer and print results if verbosity higher than 0.\n",
    "    end = time.time()\n",
    "    if verbose > 0:\n",
    "        name = str(estimator).split(\".\")[-1].split(\"'\")[0]\n",
    "        print(f'{name} \\nBest Score: {grid.best_score_} \\nBest Params: {grid.best_params_} ')\n",
    "        print(f'\\nest Estimator: {grid.best_estimator_}')\n",
    "        print(f'\\nTime Elapsed: {((end - start))/60} minutes')\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def random_pipe(estimator, params, X_train, y_train, X_test, y_test, n_components='mle',\n",
    "                scaler=StandardScaler(), n_iter=10, random_state=42, cv=3, verbose=2, n_jobs=-1):\n",
    "\n",
    "    \"\"\"\n",
    "    Fits pipeline and performs a randomized grid search with cross validation.\n",
    "    \n",
    "    Parameters:\n",
    "    --------------\n",
    "    estimator: estimator object,\n",
    "            This is assumed to implement the scikit-learn estimator interface. \n",
    "            Ex. sklearn.svm.SVC \n",
    "    params: dict, \n",
    "            Dictionary with parameters names (string) as keys and distributions or\n",
    "             lists of parameters to try. Distributions must provide a rvs method for \n",
    "             sampling (such as those from scipy.stats.distributions). \n",
    "             If a list is given, it is sampled uniformly.\n",
    "            MUST BE IN FORM: 'clf__param_'. ex. 'clf__C':[1, 10, 100]\n",
    "    n_components: int, float, None or str. default='mle'\n",
    "            Number of components to keep. if n_components is not set all components are kept.\n",
    "            If n_components == 'mle'  Minka’s MLE is used to guess the dimension. For PCA.\n",
    "    X_train, y_train, X_test, y_test: \n",
    "            training and testing data to fit, test to model\n",
    "    scaler: sklearn.preprocessing class instance,\n",
    "            MUST BE IN FORM: StandardScaler(), (default=StandardScaler())\n",
    "    n_iter: int,\n",
    "            Number of parameter settings that are sampled. n_iter trades off \n",
    "            runtime vs quality of the solution.\n",
    "    random_state: int, RandomState instance or None, optional, default=42\n",
    "            Pseudo random number generator state used for random uniform sampling from lists of \n",
    "            possible values instead of scipy.stats distributions. If int, random_state is the \n",
    "            seed used by the random number generator; If RandomState instance, random_state \n",
    "            is the random number generator; If None, the random number generator is the \n",
    "            RandomState instance used by np.random.\n",
    "    cv:  int, cross-validation generator or an iterable, optional\n",
    "            Determines the cross-validation splitting strategy. Possible inputs for cv are:\n",
    "                None, to use the default 3-fold cross validation,\n",
    "                integer, to specify the number of folds in a (Stratified)KFold,\n",
    "                CV splitter,\n",
    "                An iterable yielding (train, test) splits as arrays of indices.\n",
    "    verbose : int,\n",
    "            Controls the verbosity: the higher, the more messages.\n",
    "    n_jobs : int or None, optional (default = -1)\n",
    "            Number of jobs to run in parallel. None means 1 unless in a joblib.parallel_backend \n",
    "            context. -1 means using all processors. See Glossary for more details.\n",
    "    \n",
    "     Returns:\n",
    "    ------------\n",
    "        dictionary:\n",
    "                keys are: 'test_score' , 'best_accuracy' (training validation score),\n",
    "                'best_params', 'best_estimator', 'results'\n",
    "    \n",
    "    \"\"\"\n",
    "    # Start timer\n",
    "    start = time.time()\n",
    "\n",
    "    # Create dictioinary for storing results.\n",
    "    results = {}\n",
    "    # Instantiate Pipeline object.\n",
    "    pipe = Pipeline([('scaler', scaler),\n",
    "                        ('pca', PCA(n_components=n_components,random_state=random_state)),\n",
    "                        ('clf', estimator(random_state=random_state))])\n",
    "\n",
    "    # Fit pipeline to training data.                    \n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    # Instantiate RandomizedSearchCV object.\n",
    "    grid = RandomizedSearchCV(estimator = pipe,\n",
    "        param_distributions = params,\n",
    "        n_iter = n_iter,\n",
    "        scoring = 'accuracy',\n",
    "        cv = cv, verbose = verbose, n_jobs=n_jobs, return_train_score = True)\n",
    "\n",
    "    # Fit gridsearch object to training data.\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    # Store Test scores in results dictionary.\n",
    "    results['test_score'] = grid.score(X_test, y_test)\n",
    "    results['best_accuracy'] = grid.best_score_\n",
    "    results['best_params'] = grid.best_params_\n",
    "    results['best_estimator'] = grid.best_estimator_\n",
    "    results['results'] = grid.cv_results_\n",
    "\n",
    "    # End timer\n",
    "    end = time.time()\n",
    "\n",
    "    # print concise results if verbosity greater than 0.\n",
    "    if verbose > 0:\n",
    "        name = str(estimator).split(\".\")[-1].split(\"'\")[0]\n",
    "        print(f'{name} \\nBest Score: {grid.best_score_} \\nBest Params: {grid.best_params_} ')\n",
    "        print(f'\\nBest Estimator: {grid.best_estimator_}')\n",
    "        print(f'\\nTime Elapsed: {((end - start))/60} minutes')\n",
    "    \n",
    "    return results\n",
    "\n",
    "def compare_pipes(config_dict, X_train, y_train, X_test, y_test, n_components='mle',\n",
    "                 search='random',scaler=StandardScaler(), n_iter=10, random_state=42,\n",
    "                  cv=3, verbose=2, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Runs any number of estimators through pipeline and gridsearch(exhaustive or radomized) with cross validations, \n",
    "    can print dataframe with scores, returns dictionary of all results.\n",
    "\n",
    "    Parameters:\n",
    "    --------------\n",
    "    estimator: estimator object,\n",
    "            This is assumed to implement the scikit-learn estimator interface. \n",
    "            Ex. sklearn.svm.SVC \n",
    "    params: dict, or list of dictionaries if using GridSearchcv, cannot pass lists if search='random\n",
    "            Dictionary with parameters names (string) as keys and distributions or\n",
    "             lists of parameters to try. Distributions must provide a rvs method for \n",
    "             sampling (such as those from scipy.stats.distributions). \n",
    "             If a list is given, it is sampled uniformly.\n",
    "            MUST BE IN FORM: 'clf__param_'. ex. 'clf__C':[1, 10, 100]\n",
    "    X_train, y_train, X_test, y_test: \n",
    "            training and testing data to fit, test to model\n",
    "    n_components: int, float, None or str. default='mle'\n",
    "            Number of components to keep. if n_components is not set all components are kept.\n",
    "            If n_components == 'mle'  Minka’s MLE is used to guess the dimension. For PCA.\n",
    "    search: str, 'random' or 'grid',\n",
    "            Type of gridsearch to execute, 'random' = RandomizedSearchCV,\n",
    "            'grid' = GridSearchCV.\n",
    "    scaler: sklearn.preprocessing class instance,\n",
    "            MUST BE IN FORM: StandardScaler(), (default=StandardScaler())\n",
    "    n_iter: int,\n",
    "            Number of parameter settings that are sampled. n_iter trades off \n",
    "            runtime vs quality of the solution.\n",
    "    random_state: int, RandomState instance or None, optional, default=42\n",
    "            Pseudo random number generator state used for random uniform sampling from lists of \n",
    "            possible values instead of scipy.stats distributions. If int, random_state is the \n",
    "            seed used by the random number generator; If RandomState instance, random_state \n",
    "            is the random number generator; If None, the random number generator is the \n",
    "            RandomState instance used by np.random.\n",
    "    cv:  int, cross-validation generator or an iterable, optional\n",
    "            Determines the cross-validation splitting strategy. Possible inputs for cv are:\n",
    "                None, to use the default 3-fold cross validation,\n",
    "                integer, to specify the number of folds in a (Stratified)KFold,\n",
    "                CV splitter,\n",
    "                An iterable yielding (train, test) splits as arrays of indices.\n",
    "    verbose : int,\n",
    "            Controls the verbosity: the higher, the more messages.\n",
    "    n_jobs : int or None, optional (default = -1)\n",
    "            Number of jobs to run in parallel. None means 1 unless in a joblib.parallel_backend \n",
    "            context. -1 means using all processors. See Glossary for more details.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #Start timer\n",
    "    begin = time.time()\n",
    "    # CreateDictionary to store results from each grid search. Create list for displaying results.\n",
    "    compare_dict = {}\n",
    "    df_list = [['estimator', 'Test Score', 'Best Accuracy Score']]\n",
    "    # Loop through dictionary instantiate pipeline and grid search on each estimator.\n",
    "    for k, v in config_dict.items():\n",
    "\n",
    "        # perform RandomizedSearchCV.\n",
    "        if search == 'random':\n",
    "\n",
    "            # Assert params are in correct form, as to not raise error after running search.\n",
    "            if type (v) == list:\n",
    "                raise ValueError(\"'For random search, params must be dictionary, not list \")\n",
    "            else:\n",
    "                results = random_pipe(k, v, X_train, y_train, X_test, y_test, n_components, \n",
    "                                    scaler, n_iter, random_state, cv, verbose, n_jobs)\n",
    "        # Perform GridSearchCV.\n",
    "        elif search == 'grid':\n",
    "            results = pipe_search(k, v, X_train, y_train, X_test, y_test, n_components, \n",
    "                                        scaler, random_state, cv, verbose, n_jobs )\n",
    "\n",
    "        # Raise error if grid parameter not specified.\n",
    "        else:\n",
    "            raise ValueError(f\"search expected 'random' or 'grid' instead got{search}\")\n",
    "\n",
    "        # append results to display list and dictionary.\n",
    "        name = str(k).split(\".\")[-1].split(\"'\")[0]\n",
    "        compare_dict[name] = results\n",
    "        df_list.append([name, results['test_score'], results['best_accuracy']])\n",
    "\n",
    "    # Display results if verbosity greater than 0.\n",
    "    finish = time.time()\n",
    "    if verbose > 0:\n",
    "        print(f'\\nTotal runtime: {((finish - begin)/60)}')\n",
    "        display(list2df(df_list))\n",
    "    \n",
    "    return compare_dict\n",
    "HTML(f\"<style>{CSS}</style>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env-ext",
   "language": "python",
   "name": "learn-env-ext"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
